# 动手学深度学习笔记-李沐

论坛：https://discuss.d2l.ai/c/chinese-version/16

答案：https://datawhalechina.github.io/d2l-ai-solutions-manual/#/

## 前置基础

### 节省内存

花费内存：

![image-20231021202024038](https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231021202024038.png)

节省内存：

![image-20231021202003674](https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231021202003674.png)

或

![image-20231021202122401](https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231021202122401.png)

### axis操作

axis=0相当于把shape中第0个的维度去掉（降维），如果增加一个参数keepdims=True，则将该维度设为1（维度不变）

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231021214931540.png" alt="image-20231021214931540" style="zoom:80%;" />

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231021215626895.png" alt="image-20231021215626895" style="zoom:80%;" />

### pytorch矩阵向量计算的语法

```python
torch.mm(A, B) #两个矩阵点乘
torch.mv(A, x) #矩阵和向量点乘
torch.dot(x, y) #向量和向量点乘
torch.norm(u) #求向量u的L2范数
torch.abs(u).sum() #求向量u的L1范数
torch.norm(torch.ones((4, 9))) #求矩阵的Frobenius范数（所有元素的平方和开根号）
```

### 矩阵计算（求导）

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231023214537662.png" alt="image-20231023214537662" style="zoom:80%;" />

1. 标量对向量求导

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231023213409541.png" alt="image-20231023213409541" style="zoom:80%;" />

   如$\frac{\partial (x_1^2+2x_2^2)}{\partial \boldsymbol X}=[2x_1,4x_2]$

   下面是几个例子：

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231023214019015.png" alt="image-20231023214019015" style="zoom:80%;" />

   上图中，$<\boldsymbol u,\boldsymbol v>$表示二者的内积

2. 向量对标量求导

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231023214520103.png" alt="image-20231023214520103" style="zoom:80%;" />

   $\frac{\partial \boldsymbol y}{\partial x}$表示列向量，$\frac{\partial y}{\partial \boldsymbol x}$表示行向量，这种布局方法成为分子布局法（以分子的维度为准则）

3. 向量对向量求导

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231023215011271.png" alt="image-20231023215011271" style="zoom:80%;" />

   下面是几个例子：

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231023215151747.png" alt="image-20231023215151747" style="zoom:80%;" />

4. 矩阵对矩阵求导

### 自动求导

#### 向量求导的链式法则

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231024162048000.png" alt="image-20231024162048000" style="zoom:80%;" />

1. 向量求导举例：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231024162510520.png" alt="image-20231024162510520" style="zoom:80%;" />

2. 矩阵求导举例：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231024163121326.png" alt="image-20231024163121326" style="zoom:80%;" />

​	注意：向量的模就相当于向量的内积$<\boldsymbol u,\boldsymbol v>$，同时$<\boldsymbol u,\boldsymbol v>=\boldsymbol u^T \boldsymbol v$

#### 导数的近似解

数理上的求导，是求解出导数的解析解，而实际当中几乎不可能知道函数的具体表达式，因此只能求**某一数值下的导数的近似解**，使用如下公式：$$\frac{\partial f(x)}{\partial x}=\displaystyle \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}$$

#### 计算图

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231024164610510.png" alt="image-20231024164610510" style="zoom:80%;" />

#### 自动求导的两种模式

1. 正向累积

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231024164923342.png" alt="image-20231024164923342" style="zoom:80%;" />

   正向累积就是从开始到结尾依次计算，计算复杂度是O(n)，因为其计算过程中不需要存储中间值，因此其内存复杂度是O(1)。

2. 反向传递

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231024164933442.png" alt="image-20231024164933442" style="zoom:80%;" />

   反向传递就是从结尾反向计算到开始，计算复杂度和内存复杂度都是O(n)，因为需要存储中间值。

## 深度学习基础

### 线性神经网络

计时器函数

```python
class Timer:  #@save
    """记录多次运行时间"""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """启动计时器"""
        self.tik = time.time()

    def stop(self):
        """停止计时器并将时间记录在列表中"""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """返回平均时间"""
        return sum(self.times) / len(self.times)

    def sum(self):
        """返回时间总和"""
        return sum(self.times)

    def cumsum(self):
        """返回累计时间"""
        return np.array(self.times).cumsum().tolist()

timer = Timer() #自动开始计时器，也可以使用timer.start()来开始
d = a + b
f'{timer.stop():.5f} sec' #执行timer.stop()会自动停止计时，并返回运行时长
```



#### Softmax 回归

1. 分类问题独热编码

2. 网络架构

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231101105339803.png" alt="image-20231101105339803" style="zoom:67%;" />
   $$
   \begin{aligned}
   o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
   o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
   o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
   \end{aligned}
   $$
   通过向量表示为：$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$

3. 全连接层的参数开销：

   对于任何具有$d$个输入和$q$个输出的全连接层，参数开销为$\mathcal{O}(dq)$，这个数字在实践中可能高得令人望而却步。幸运的是，将$d$个输入转换为$q$个输出的成本可以减少到$\mathcal{O}(\frac{dq}{n})$，其中超参数$n$可以由我们灵活指定。

4. 公式
   $$
   \hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
   $$

5. 小批量样本的矢量化

   为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算。
   假设我们读取了一个批量的样本$\mathbf{X}$，其中特征维度（输入数量）为$d$，批量大小为$n$。
   此外，假设我们在输出中有$q$个类别。
   那么小批量样本的特征为$\mathbf{X} \in \mathbb{R}^{n \times d}$，权重为$\mathbf{W} \in \mathbb{R}^{d \times q}$，偏置为$\mathbf{b} \in \mathbb{R}^{1\times q}$。
   softmax回归的矢量计算表达式为：
   $$
   \begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned}
   $$
   相对于一次处理一个样本，小批量样本的矢量化加快了$\mathbf{X}和\mathbf{W}$的矩阵-向量乘法。
   由于$\mathbf{X}$中的每一行代表一个数据样本，那么softmax运算可以*按行*（rowwise）执行：
   对于$\mathbf{O}$的每一行，我们先对所有项进行幂运算，然后通过求和对它们进行标准化。

6. 损失函数

   softmax函数给出了一个向量$\hat{\mathbf{y}}$，我们可以将其视为“对给定任意输入$\mathbf{x}$的每个类的条件概率”。
   例如，$\hat{y}_1$=$P(y=\text{猫} \mid \mathbf{x})$。假设整个数据集$\{\mathbf{X}, \mathbf{Y}\}$具有$n$个样本，其中索引$i$的样本由特征向量$\mathbf{x}^{(i)}$和独热标签向量$\mathbf{y}^{(i)}$组成。我们可以将估计值与实际值进行比较：

   对数似然：$$P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).$$

   取负对数似然，表示为求和的形式：
   $$
   -\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
   = \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)})​
   $$
   损失函数为：
   $$
   l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j
   $$
   称为交叉熵损失。其中$y_j$为独热标签中某一项的取值，$\hat{y}_j$为对应的预测概率。

   将$\hat{y}_j$带入得：
   $$
   \begin{aligned}
   l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
   &= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
   &= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
   \end{aligned}
   $$
   考虑相对于任何未规范化的预测$o_j$的导数，我们得到：
   $$
   \partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
   $$

### 多层感知机

多层感知机与Softmax回归相比，仅仅是在中间加了一层（或多层）神经元，隐含层的激活函数一般选择ReLU。通常，我们选择2的若干次幂作为层的宽度（神经元数量）。 因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。
$$
\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}
$$
<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231106185303881.png" alt="image-20231106185303881" style="zoom:80%;" />

#### 激活函数

1. ReLU

   **$$\operatorname{ReLU}(x) = \max(x, 0)$$**

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231106185058094.png" alt="image-20231106185058094" style="zoom:67%;" />

   $$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)$$

2. sigmoid

   $$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}$$

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231106185157534.png" alt="image-20231106185157534" style="zoom:67%;" />

3. tanh

   $$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}$$

   <img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231106185223843.png" alt="image-20231106185223843" style="zoom:67%;" />

#### 前向传播与反向传播的数学推导

详细见：4.7 前向传播、反向传播和计算图（P162）



习题中，有**训练和预测时的内存占用推导，权重+偏置项的反向传播推导**，地址如下：https://datawhalechina.github.io/d2l-ai-solutions-manual/#/ch04/ch04?id=_47-%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad%e3%80%81%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%92%8c%e8%ae%a1%e7%ae%97%e5%9b%be

### 过拟合、欠拟合

https://www.bilibili.com/video/BV1kX4y1g7jp/?spm_id_from=trigger_reload&vd_source=cc795b3254268ba2ea72721f001aac4b

### 权重衰退（岭回归、正则项）

对于一个线性回归的损失函数：
$$
L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2
$$
在后面加上惩罚项，最小化这个结果：
$$
L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2
$$
权重更新过程：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231107211637929.png" alt="image-20231107211637929" style="zoom:67%;" />



### 丢弃法（dropout）

#### 基本概述

暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为**暂退法**，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。因此在模型的训练阶段可以加入dropout，但是在测试阶段就不能加入dropout了。

在毕晓普的工作中，他将高斯噪声添加到线性模型的输入中。在每次训练迭代中，他将从均值为零的分布$\epsilon \sim \mathcal{N}(0,\sigma^2)$采样噪声添加到输入$\mathbf{x}$，从而产生扰动点$\mathbf{x}' = \mathbf{x} + \epsilon$，预期是$E[\mathbf{x}'] = \mathbf{x}$。

在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。
换言之，每个中间活性值$h$以**暂退概率**$p$由随机变量$h'$替换，如下所示：
$$
\begin{aligned}
h' =
\begin{cases}
    0 & \text{ 概率为 } p \\
    \frac{h}{1-p} & \text{ 其他情况}
\end{cases}
\end{aligned}
$$
根据此模型的设计，其期望值保持不变，即$E[h'] = h$。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231108142847383.png" alt="image-20231108142847383" style="zoom:67%;" />

#### 代码实现

李沐对于dropout的代码实现值得学习：

```python
import torch
from torch import nn
from d2l import torch as d2l


def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
        return X
    # 先生成一个与输入X形状相同的服从0-1均匀分布的随机变量矩阵，再与丢弃率做判断，得到布尔值，将布尔值转为浮点型
    mask = (torch.rand(X.shape) > dropout).float()
    '''使用乘法计算而不是以选值（如X[mask]）的方式将特定的值变为0，这样可以很好地利用GPU的算力，否则计算将会很慢'''
    return mask * X / (1.0 - dropout) #矩阵相乘就可以得到哪些神经元保留，哪些神经元变为0，最后再对保留的神经元进行扩大
```

### 数值稳定性和模型初始化

#### 梯度消失和梯度爆炸

考虑一个具有$L$层、输入$\mathbf{x}$和输出$\mathbf{o}$的深层网络。每一层$l$由变换$f_l$定义，该变换的参数为权重$\mathbf{W}^{(l)}$，其隐藏变量是$\mathbf{h}^{(l)}$（令 $\mathbf{h}^{(0)} = \mathbf{x}$）。我们的网络可以表示为：

$$
\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \text{ 因此 } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x})
$$
如果所有隐藏变量和输入都是向量，我们可以将$\mathbf{o}$关于任何一组参数$\mathbf{W}^{(l)}$的梯度写为下式：

$$
\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}
$$
换言之，该梯度是$L-l$个矩阵$\mathbf{M}^{(L)} \cdot \ldots \cdot \mathbf{M}^{(l+1)}$与梯度向量 $\mathbf{v}^{(l)}$的乘积。因此只要其中某些值过大或者过小，就会导致梯度爆炸或者梯度消失。

**梯度爆炸（gradient exploding）**： 参数更新过大，破坏了模型的稳定收敛。**梯度消失（gradient vanishing）**： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。

sigmoid函数在0附近的梯度最大，而距离0越远，梯度的数值越小，这都会导致**梯度消失**：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231119145959438.png" alt="image-20231119145959438" style="zoom:80%;" />

生成100个高斯随机矩阵，并将它们与某个初始矩阵（均值为0方差为1的正态分布）相乘，这是可能会发生**梯度爆炸**：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231119151206496.png" alt="image-20231119151206496" style="zoom:80%;" />

#### 如果所有参数全部初始化为一个常数会发生什么？

如果我们将隐藏层的所有参数初始化为$\mathbf{W}^{(1)} = c$，$c$为常量，会发生什么？

在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数，产生相同的激活，该激活被送到输出单元。在反向传播期间，根据参数$\mathbf{W}^{(1)}$对输出单元进行微分，得到一个梯度，其元素都取相同的值。因此，在基于梯度的迭代（例如，小批量随机梯度下降）之后，$\mathbf{W}^{(1)}$的所有元素仍然采用相同的值。这样的迭代永远不会**打破对称性**，我们可能永远也无法实现网络的表达能力。隐藏层的行为就好像只有一个单元。请注意，虽然小批量随机梯度下降不会打破这种对称性，但**暂退法正则化**可以打破对称性，**随机初始化**也能打破这种对称性。

#### Xavier初始化

如果是从均值为0，方差为1的正态分布中取得随机数，作为模型的初始权重，那么**输出的方差很可能会增大**，解释如下：

对于某些*没有非线性*的全连接层输出（例如，隐藏变量）$o_{i}$的尺度分布。对于该层$n_\mathrm{in}$输入$x_j$及其相关权重$w_{ij}$，输出由下式给出

$$
o_{i} = \sum_{j=1}^{n_\mathrm{in}} w_{ij} x_j
$$
权重$w_{ij}$都是从同一分布中独立抽取的。此外，让我们假设该分布具有零均值和方差$\sigma^2$。现在，让我们假设层$x_j$的输入也具有零均值和方差$\gamma^2$，并且它们独立于$w_{ij}$并且彼此独立。在这种情况下，我们可以按如下方式计算$o_i$的平均值和方差：
$$
\begin{aligned}
    E[o_i] & = \sum_{j=1}^{n_\mathrm{in}} E[w_{ij} x_j] \\&= \sum_{j=1}^{n_\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\
    \mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\
        & = n_\mathrm{in} \sigma^2 \gamma^2.
\end{aligned}
$$

保持方差不变的一种方法是设置$n_\mathrm{in} \sigma^2 = 1$。使用与前向传播相同的推断，我们可以看到，除非$n_\mathrm{out} \sigma^2 = 1$，否则梯度的方差可能会增大，其中$n_\mathrm{out}$是该层的输出的数量。为同时满足这两个条件，只需满足：
$$
\begin{aligned}
\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ 或等价于 }
\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.
\end{aligned}
$$
这就是**Xavier初始化**的基础，这种初始化方式可以**根据输入和输出维度自适应地选择合适的初始范围**。通常，**Xavier初始化**从均值为零，方差$\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$的高斯分布中采样权重。我们也可以从均匀分布中获得随机数，只需要将公式中的$\sigma^2$转换为均匀分布的方差即可。

## 深度学习计算（代码）

### 自定义层和块

```python
#简洁实现-----------------------------------------------------------------------
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
X = torch.rand(2, 20)
net(X)

#自定义实现---------------------------------------------------------------------
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
net = MLP()
net(X)

#自定义Sequential类-------------------------------------------------------------
#这包含以下两个关键点：
#1.一种将块逐个追加到列表中的函数；
#2.一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。
class FixedHiddenMLP(nn.Module):
    '''该类用于展示forward部分可以任意自定义计算方式，导数的计算是自动的'''
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
net = FixedHiddenMLP()
net(X)

class NestMLP(nn.Module):
    '''该类将传入的数据输入到一个多层感知机中，多层感知机的结果再输入到了一个线性层中，最后输出线性层的结果'''
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP()) #上述定义好的层均可以在此重新组合成为一个神经网络
chimera(X)
```

### 参数管理

本节主要包含以下几点内容：

- 访问参数，用于调试、诊断和可视化；
- 参数初始化；
- 在不同模型组件间共享参数。

#### 访问参数

```python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)

net[2].state_dict() #查看第2层的weight和bias
net[2].weight.data #查看第2层的weight的值
net[2].weight.grad == None #查看是否有梯度，如果没有调用反向传播，就没有梯度

print(*[(name, param.shape) for name, param in net.named_parameters()]) #查看所有层的weight和bias，以及他们的形状
net.state_dict() #查看所有层的所有信息
```

#### 参数的初始化

```python
#内置初始化------------------------------
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01) #将所有权重参数初始化为标准差为0.01的高斯随机变量
        nn.init.zeros_(m.bias) #将偏置参数设置为0
net.apply(init_normal)

def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1) #将所有参数初始化为给定的常数,1
        nn.init.zeros_(m.bias)
net.apply(init_constant)

#对某些块应用不同的初始化方法
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)
net[0].apply(init_xavier)
net[2].apply(init_42)

#自定义初始化------------------------------
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5
net.apply(my_init)
```

#### 不同模型组件间共享参数

```python
shared = nn.Linear(8, 8) #我们需要给共享层一个名称，以便可以引用它的参数
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1)) #这样所有shared层的初始参数将会相同
net(X)
'''
Q:这里有一个问题：当参数绑定时，梯度会发生什么情况？ 
A:答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。
'''
```

### 读写数据

#### 加载和保存张量

```python
x = torch.arange(4)
torch.save(x, 'x-file') #保存
torch.load('x-file') #读取

y = torch.zeros(4)
torch.save([x, y],'x-files') #保存 张量列表
x2, y2 = torch.load('x-files') #读取

mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict') #保存 张量字典
mydict2 = torch.load('mydict') #读取
```

#### 加载和保存模型参数

```python
#训练模型并保存模型参数
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))
net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

torch.save(net.state_dict(), 'mlp.params') #保存模型参数

clone = MLP() #这里直接克隆了上面的MLP类，实际当中需要编写与原始架构完全相同的代码
clone.load_state_dict(torch.load('mlp.params')) #读取模型参数
```

### 使用GPU

张量的创建和计算默认在CPU上进行，一般情况下，需要将各个操作对象放在同一个CPU或者GPU上。

```python
torch.cuda.device_count() #查看可用的GPU数量

x = torch.tensor([1, 2, 3]) #在CPU上创建张量
X = torch.ones(2, 3, device=torch.device(type='cuda', index=0)) #在第一个GPU上创建张量
```

如何将不同GPU上的数据进行相加？

由于`Y`位于第二个GPU上，所以我们需要将`X`移到那里， 然后才能执行相加运算。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231122222714481.png" alt="image-20231122222714481" style="zoom: 80%;" />

> 不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy的ndarray中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。

```python
#神经网络在GPU上创建
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=torch.device(type='cuda', index=0))
```

## 卷积神经网络

### 卷积层原理

==对全连接层使用平移不变性和局部性，就能得到卷积层==

我们提出下面两个原则，从而帮助我们设计适合于计算机视觉的神经网络架构。

1. **平移不变性（translation invariance）**：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
2. **局部性（locality）**：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

如何满足上述两个原则？

1. 平移不变性

   使用$[\mathbf{X}]_{i, j}$和$[\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中位置（$i$,$j$）处的像素。下面式子表示第i行j列的像素点以及权重V的加权乘积，a和b表示在该像素点周围取值的范围，此处a和b取值范围没有限制：
   $$
   \begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +
   \sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b} \end{aligned}
   $$
   令$[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$使不同像素点处的权重值相同：
   $$
   [\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b} [\mathbf{X}]_{i+a, j+b}
   $$
   
2. 局部性

   再令a和b取值范围固定，只在该像素点的周围取值：
   $$
   [\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}
   $$

对于彩色图片，每一个像素点有RGB三个值，比如一个图像包含$1024 \times 1024 \times 3$个像素。前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。因此，我们将$\mathsf{X}$索引为$[\mathsf{X}]_{i, j, k}$。由此卷积相应地调整为$[\mathsf{V}]_{a,b,c}$，而不是$[\mathbf{V}]_{a,b}$。由于输入图像是三维的，我们的隐藏表示$\mathsf{H}$也最好采用三维张量。为了支持输入$\mathsf{X}$和隐藏表示$\mathsf{H}$中的多个通道，我们可以在$\mathsf{V}$中添加第四个坐标，即$[\mathsf{V}]_{a, b, c, d}$，最终得到：
$$
[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c}
$$
其中隐藏表示$\mathsf{H}$中的索引$d$表示输出**通道**，而随后的输出将继续以三维张量$\mathsf{H}$作为输入进入下一个卷积层。所以，可以定义具有多个通道的卷积层，而其中$\mathsf{V}$是该卷积层的权重。

用一次卷积核操作之后，得到的矩阵维数大小可以由下面的公式（例子）来计算。假设输入形状为$n_h\times n_w$，卷积核形状为$k_h\times k_w$，那么输出形状将是$(n_h-k_h+1) \times (n_w-k_w+1)$：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231123222212498.png" alt="image-20231123222212498" style="zoom: 80%;" />

### 卷积层的手动实现

严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是*互相关运算*（cross-correlation），而不是卷积运算。下面是一维和三维的交叉相关计算表示：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231123214809684.png" alt="image-20231123214809684" style="zoom:80%;" />

卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是**卷积核权重和标量偏置**。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。

如何实现上述的卷积运算过程，将其作为卷积层？下面的代码实现了二维互相关运算和卷积层。

```python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):
    """计算二维互相关运算"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y

class Conv2D(nn.Module):
    '''自定义一个类，作为卷积层'''
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

### 填充(padding)和步幅(stride)

#### 填充

为什么要填充？因为不加填充的话，输出的维数会越来越小，导致神经网络没法太深。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231124153906291.png" alt="image-20231124153906291" style="zoom:80%;" />

通常，如果我们添加$p_h$行填充（大约一半在顶部，一半在底部）和$p_w$列填充（左侧大约一半，右侧一半），则输出形状将为

$$
(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)
$$
这意味着输出的高度和宽度将分别增加$p_h$和$p_w$。一般我们设置$p_h=k_h-1$和$p_w=k_w-1$，使输入和输出**具有相同的高度和宽度**。这样可以在构建网络时更容易地预测每个图层的输出形状。

假设$k_h$是奇数，我们将在高度的两侧填充$p_h/2$行。如果$k_h$是偶数，则一种可能性是在输入顶部填充$\lceil p_h/2\rceil$行，在底部填充$\lfloor p_h/2\rfloor$行。同理，我们填充宽度的两侧。

卷积神经网络中卷积核的高度和宽度通常为奇数，例如**1、3、5或7**。选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。

代码实现：

```python
import torch
from torch import nn


# 为了方便起见，我们定义了一个计算卷积层的函数。
# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数
def comp_conv2d(conv2d, X):
    X = X.reshape((1, 1) + X.shape) #这里的（1，1）表示批量大小和通道数都是1
    Y = conv2d(X)
    return Y.reshape(Y.shape[2:])# 省略前两个维度：批量大小和通道

# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X)

#当卷积核的高度和宽度不同时，我们可以[填充不同的高度和宽度]，使输出和输入具有相同的高度和宽度
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X)
```

#### 步幅

为什么要加步幅？

如果输入的维度太大，那么等待维数减小到比较小，需要花很长时间（神经网络很深）那么可以使得卷积核的移动距离不为1，可以将步幅调整为2或者更大，这样可以加快维数的较小。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231124160107257.png" alt="image-20231124160107257" style="zoom:80%;" />

通常，当垂直步幅为$s_h$、水平步幅为$s_w$时，输出形状为

$$
\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor
$$
如果我们设置了$p_h=k_h-1$和$p_w=k_w-1$，则输出形状将简化为
$$
\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor
$$
更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为$(n_h/s_h) \times (n_w/s_w)$。

#### QA

Q:现在已经有很多经典的网络结构了，我们平时使用的时候，需要自己设计卷积核的大小吗，还是说直接套用经典的网络结构？

> 一般套用经典的网络结构，在别人的基础上做修改。

Q:卷积核的大小、填充、步幅三个超参数重要性如何？

> 一般来说，卷积核的大小是最关键的超参数，填充一般取默认的，步幅取决于希望将神经网络控制在什么样的复杂程度。

### 多输入和多输出通道

每个RGB输入图像具有$3\times h\times w$的形状，我们将这个大小为$3$的轴称为**通道（channel）**维度。

#### 多输入通道

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231124161800051.png" alt="image-20231124161800051" style="zoom:80%;" />

代码实现：

```python
import torch
from d2l import torch as d2l

def corr2d_multi_in(X, K):
    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))

X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])
K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])

corr2d_multi_in(X, K)
#tensor([[ 56.,  72.],
#        [104., 120.]])
```



#### 多输出通道

到目前为止，不论有多少输入通道，我们还只有一个输出通道。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。**多个通道实际就是不同的卷积核对同一对象进行加权求和，利用不同的权重表达了不同的特征。**

用$c_i$和$c_o$分别表示输入和输出通道的数目，并让$k_h$和$k_w$为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为$c_i\times k_h\times k_w$的卷积核张量，这样卷积核的形状是$c_o\times c_i\times k_h\times k_w$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。

代码实现：
```python
def corr2d_multi_in_out(X, K):
    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。
    # 最后将所有结果都叠加在一起
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
K = torch.stack((K, K + 1, K + 2), 0) #实际就是将单个卷积核K加1和加2，最终得到3个卷积核
# K的结果如下 
tensor([[[[0., 1.],
          [2., 3.]],

         [[1., 2.],
          [3., 4.]]],


        [[[1., 2.],
          [3., 4.]],

         [[2., 3.],
          [4., 5.]]],


        [[[2., 3.],
          [4., 5.]],

         [[3., 4.],
          [5., 6.]]]])
corr2d_multi_in_out(X, K) #分别将通道与三个卷积核相乘，得到三个输出结果
tensor([[[ 56.,  72.],
         [104., 120.]],

        [[ 76., 100.],
         [148., 172.]],

        [[ 96., 128.],
         [192., 224.]]])
```

#### $1\times 1$ 卷积层

我们可以将$1\times 1$卷积层看作在每个像素位置应用的全连接层，以$c_i$个输入值转换为$c_o$个输出值。因为这仍然是一个卷积层，所以跨像素的权重是一致的。同时，$1\times 1$卷积层需要的权重维度为$c_o\times c_i$，再额外加上一个偏置。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231124202811573.png" alt="image-20231124202811573" style="zoom:80%;" />

* 当以每像素为基础应用时，$1\times 1$卷积层相当于全连接层。上图表示的卷积层其实就相当于输入数据的维度是3、输出层的神经元个数是2、有9组数据的情况。
* $1\times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。

### 池化层pooling（汇聚层）

* 池化层返回窗口中最大或平均值
* 缓解卷积层的位置敏感性
* 同样有窗口大小、填充和步幅作为超参数
* 池化层一般添加在卷积层的后面

1. **最大池化层**如下图所示

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231124204908980.png" alt="image-20231124204908980" style="zoom:80%;" />

2. 步幅和填充

与卷积层一样，池化层也有步幅和填充。默认情况下，(深度学习框架中的步幅与汇聚窗口的大小相同)。因此，如果我们使用形状为(3, 3)的汇聚窗口，那么默认情况下，我们得到的步幅形状为(3, 3)。

```python
#代码实现
pool2d = nn.MaxPool2d(3)
pool2d(X)

pool2d = nn.MaxPool2d(3, padding=1, stride=2) #手动设计步幅和填充
pool2d(X)
```

3. 多个通道

在处理多通道输入数据时，**汇聚层在每个输入通道上单独运算**，而不是像卷积层一样在通道上对输入进行汇总。 这意味着汇聚层的输出通道数与输入通道数相同。

### LeNet

![image-20231125082829170](https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231125082829170.png)

```python
#LeNet核心代码
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5,padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.ReLU(),
    nn.Linear(120, 84), nn.ReLU(),
    nn.Linear(84, 10))

'''用于打印每一层的输出数据的维数，用来检查前后维数是否对齐'''
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)

#Conv2d output shape: 	 torch.Size([1, 6, 28, 28])
#ReLU output shape: 	 torch.Size([1, 6, 28, 28])
#MaxPool2d output shape: 	 torch.Size([1, 6, 14, 14])
#Conv2d output shape: 	 torch.Size([1, 16, 10, 10])
#ReLU output shape: 	 torch.Size([1, 16, 10, 10])
#MaxPool2d output shape: 	 torch.Size([1, 16, 5, 5])
#Flatten output shape: 	 torch.Size([1, 400])
#Linear output shape: 	 torch.Size([1, 120])
#ReLU output shape: 	 torch.Size([1, 120])
#Linear output shape: 	 torch.Size([1, 84])
#ReLU output shape: 	 torch.Size([1, 84])
#Linear output shape: 	 torch.Size([1, 10])

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    """使用GPU计算模型在数据集上的精度"""
    if isinstance(net, nn.Module):
        net.eval()  # 设置为评估模式
        if not device:
            device = next(iter(net.parameters())).device
    # 正确预测的数量，总预测的数量
    metric = d2l.Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERT微调所需的（之后将介绍）
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """用GPU训练模型(在第六章定义)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # 训练损失之和，训练准确率之和，样本数
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
    
lr, num_epochs = 0.1, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231125162158660.png" alt="image-20231125162158660" style="zoom:80%;" />

### AlexNet

AlexNet和LeNet的设计理念非常相似，但也存在显著差异：

1. 卷积核比LeNet大很多，**卷积核的大小逐级递减，通道数目是LeNet的10倍**。
2. AlexNet比相对较小的LeNet5要**深**得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。
3. AlexNet使用**ReLU**而不是sigmoid作为其激活函数。防止梯度消失，简化模型的运算。
4. 使用**dropout和数据扩充**，增强模型的泛化能力。

AlexNet与LeNet对比图：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231125185246876.png" alt="image-20231125185246876" style="zoom: 67%;" />

```python
'''代码实现'''
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    # 这里使用一个11*11的更大窗口来捕捉对象。
    # 同时，步幅为4，以减少输出的高度和宽度。
    # 另外，输出通道的数目远大于LeNet
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), # 输入数据的通道数为1
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 使用三个连续的卷积层和较小的卷积窗口。
    # 除了最后的卷积层，输出通道的数量进一步增加。
    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
    nn.Linear(4096, 10))

'''查看每一层的输出形状'''
X = torch.randn(1, 1, 224, 224)
for layer in net:
    X=layer(X)
    print(layer.__class__.__name__,'output shape:\t',X.shape)
#Conv2d output shape:	 torch.Size([1, 96, 54, 54])
#ReLU output shape:	 	 torch.Size([1, 96, 54, 54])
#MaxPool2d output shape: torch.Size([1, 96, 26, 26])
#Conv2d output shape:	 torch.Size([1, 256, 26, 26])
#ReLU output shape:	 	 torch.Size([1, 256, 26, 26])
#MaxPool2d output shape: torch.Size([1, 256, 12, 12])
#Conv2d output shape:	 torch.Size([1, 384, 12, 12])
#ReLU output shape:	 	 torch.Size([1, 384, 12, 12])
#Conv2d output shape:	 torch.Size([1, 384, 12, 12])
#ReLU output shape:	 	 torch.Size([1, 384, 12, 12])
#Conv2d output shape:	 torch.Size([1, 256, 12, 12])
#ReLU output shape:	 	 torch.Size([1, 256, 12, 12])
#MaxPool2d output shape: torch.Size([1, 256, 5, 5])
#Flatten output shape:	 torch.Size([1, 6400])
#Linear output shape:	 torch.Size([1, 4096])
#ReLU output shape:	 	 torch.Size([1, 4096])
#Dropout output shape:	 torch.Size([1, 4096])
#Linear output shape:	 torch.Size([1, 4096])
#ReLU output shape:	 	 torch.Size([1, 4096])
#Dropout output shape:	 torch.Size([1, 4096])
#Linear output shape:	 torch.Size([1, 10])

batch_size = 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)

lr, num_epochs = 0.01, 20
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231125200036622.png" alt="image-20231125200036622" style="zoom:80%;" />

### VGG(visual geometry group)

虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。而VGG提出**块的思想**，让人们可以使用更加模块化的思维来构建神经网络。下面是VGG的主要优势和特点：

1. VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。
2. 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。
3. 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现**深层且窄的卷积（即$3 \times 3$）比较浅层且宽的卷积更有效**。

AlexNet和VGG对比图：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231125204321308.png" alt="image-20231125204321308" style="zoom:67%;" />

```python
import torch
from torch import nn
from d2l import torch as d2l

'''实现VGG块'''
def vgg_block(num_convs, in_channels, out_channels):
    '''该函数有三个参数，分别对应于卷积层的数量num_convs、输入通道的数量in_channels 和输出通道的数量out_channels'''
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))

def vgg(conv_arch):
    conv_blks = []
    in_channels = 1
    # 卷积层部分
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
        in_channels = out_channels

    return nn.Sequential(
        *conv_blks, nn.Flatten(),
        # 全连接层部分
        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 10))
net = vgg(conv_arch)

X = torch.randn(size=(1, 1, 224, 224))
for blk in net:
    X = blk(X)
    print(blk.__class__.__name__,'output shape:\t',X.shape)
#Sequential output shape:	 torch.Size([1, 64, 112, 112])
#Sequential output shape:	 torch.Size([1, 128, 56, 56])
#Sequential output shape:	 torch.Size([1, 256, 28, 28])
#Sequential output shape:	 torch.Size([1, 512, 14, 14])
#Sequential output shape:	 torch.Size([1, 512, 7, 7])
#Flatten output shape:	 torch.Size([1, 25088])
#Linear output shape:	 torch.Size([1, 4096])
#ReLU output shape:	 torch.Size([1, 4096])
#Dropout output shape:	 torch.Size([1, 4096])
#Linear output shape:	 torch.Size([1, 4096])
#ReLU output shape:	 torch.Size([1, 4096])
#Dropout output shape:	 torch.Size([1, 4096])
#Linear output shape:	 torch.Size([1, 10])

ratio = 4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]
net = vgg(small_conv_arch)

lr, num_epochs, batch_size = 0.05, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231125211214358.png" alt="image-20231125211214358" style="zoom: 80%;" />

### NiN

NiN的特点和优势：

1. NiN使用由一个卷积层和多个**$1\times 1$卷积层组成的块**。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
2. **NiN去除了容易造成过拟合的全连接层**，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。
3. 移除全连接层可**减少过拟合**，同时显著减少NiN的参数。

经验小结：

1. 全连接层很占内存，层数越多越占内存，模型越占内存越难训练。
2. 图像尺寸减半，同时通道数指数增长，可以很好地保留特征

VGG与NiN对比图：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231125213715103.png" alt="image-20231125213715103" style="zoom:80%;" />

代码实现：

```python
import torch
from torch import nn
from d2l import torch as d2l


def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())
        
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0), #将通道数从1提升至96
    nn.MaxPool2d(3, stride=2), #通道数不变，图片的尺寸减小
    nin_block(96, 256, kernel_size=5, strides=1, padding=2), #将通道数从96提升至256
    nn.MaxPool2d(3, stride=2), #通道数不变，图片的尺寸减小
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),#将通道数从256提升至384
    nn.MaxPool2d(3, stride=2), #通道数不变，图片的尺寸减小
    nn.Dropout(0.5),
    # 标签类别数是10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1), #将通道数从384减少到10
    nn.AdaptiveAvgPool2d((1, 1)), #此时有10个通道，图片尺寸为5*5，该行代码应用一个与图片大小相等的池化层卷积核，取所有像素点的平均值，分别应用于10个通道中，得到10个输出结果。
    # 将四维的输出转成二维的输出，其形状为(批量大小,10)
    nn.Flatten()) #最终将其展开，Softmax函数在交叉熵损失里面

#查看每个输出块的形状
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
#Sequential output shape:	 torch.Size([1, 96, 54, 54])
#MaxPool2d output shape:	 torch.Size([1, 96, 26, 26])
#Sequential output shape:	 torch.Size([1, 256, 26, 26])
#MaxPool2d output shape:	 torch.Size([1, 256, 12, 12])
#Sequential output shape:	 torch.Size([1, 384, 12, 12])
#MaxPool2d output shape:	 torch.Size([1, 384, 5, 5])
#Dropout output shape:	 torch.Size([1, 384, 5, 5])
#Sequential output shape:	 torch.Size([1, 10, 5, 5])
#AdaptiveAvgPool2d output shape:	 torch.Size([1, 10, 1, 1])
#Flatten output shape:	 torch.Size([1, 10])

lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231126160640193.png" alt="image-20231126160640193" style="zoom:80%;" />

### GoogLeNet

GoogLeNet 于2014年提出，GoogLeNet 论文的一个重点是解决了什么样大小的卷积核最合适的问题。论文的一个观点是，有时使用**不同大小的卷积核组合是有利的**。

特点及优势：

1. Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用$1×1$卷积层减少每像素级别上的通道维数从而降低模型复杂度。
2.  GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。
3. GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。
4. GoogLeNet继承NiN的特点，使用在每个块的后面加上最大池化层，使用全局平均池化层AdaptiveAvgPool来代替4096个神经元的全连接层，减少了模型的计算量。但是最后又加了一个全连接层，将通道数降到输出的维度数。

值得注意的是，GoogLeNet在后续经过了多次改进，添加了很多技巧（batch norm 和 残差连接），并且将网络改得更加复杂。现在应用最广泛的模型是GoogLeNet-V4。

#### Inception block

GoogLeNet 中的块称为Inception block：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231126164454973.png" alt="image-20231126164454973" style="zoom:80%;" />

代码实现：

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

class Inception(nn.Module):
    # c1--c4是每条路径的输出通道数
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 线路1，单1x1卷积层
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # 线路2，1x1卷积层后接3x3卷积层
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # 线路3，1x1卷积层后接5x5卷积层
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # 线路4，3x3最大汇聚层后接1x1卷积层
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        '''将每一条线上的卷积中间加上ReLU，组合成p1,p2,p3,p4'''
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # 在通道维度上连结输出
        return torch.cat((p1, p2, p3, p4), dim=1)
```

#### GoogLeNet整体模型

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231126165307031.png" alt="image-20231126165307031" style="zoom:80%;" />

下面使用5个模块来实现GoogLeNet：

1. 第一个模块使用64个通道、$7\times 7$卷积层

```python
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

2. 第二个模块使用两个卷积层

第一个卷积层是64个通道、$1\times 1$卷积层；第二个卷积层使用将通道数量增加三倍的$3\times 3$卷积层

```python
b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),
                   nn.ReLU(),
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

3. 第三个模块串联两个完整的Inception块

第一个Inception块的输出通道数为$64+128+32+32=256$，四个路径之间的输出通道数量比为$64:128:32:32=2:4:1:1$。
第二个和第三个路径首先将输入通道的数量分别减少到$96/192=1/2$和$16/192=1/12$，然后连接第二个卷积层。第二个Inception块的输出通道数增加到$128+192+96+64=480$，四个路径之间的输出通道数量比为$128:192:96:64 = 4:6:3:2$。
第二条和第三条路径首先将输入通道的数量分别减少到$128/256=1/2$和$32/256=1/8$。

```python
b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                   Inception(256, 128, (128, 192), (32, 96), 64),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

4. 第四模块串联了5个Inception块

其输出通道数分别是$192+208+48+64=512$、$160+224+64+64=512$、$128+256+64+64=512$、$112+288+64+64=528$和$256+320+128+128=832$。
这些路径的通道数分配和第三模块中的类似，首先是含$3×3$卷积层的第二条路径输出最多通道，其次是仅含$1×1$卷积层的第一条路径，之后是含$5×5$卷积层的第三条路径和含$3×3$最大汇聚层的第四条路径。
其中第二、第三条路径都会先按比例减小通道数。
这些比例在各个Inception块中都略有不同。

```python
b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

5. 第五模块串联了2个Inception块

第五模块包含输出通道数为$256+320+128+128=832$和$384+384+128+128=1024$的两个Inception块。
其中每条路径通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。
需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均汇聚层，将每个通道的高和宽变成1。
最后我们将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层。

```python
b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                   Inception(832, 384, (192, 384), (48, 128), 128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))
```

每个模块各个输出数据的形状：

```python
X = torch.rand(size=(1, 1, 96, 96))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
#Sequential output shape:	 torch.Size([1, 64, 24, 24])
#Sequential output shape:	 torch.Size([1, 192, 12, 12])
#Sequential output shape:	 torch.Size([1, 480, 6, 6])
#Sequential output shape:	 torch.Size([1, 832, 3, 3])
#Sequential output shape:	 torch.Size([1, 1024])
#Linear output shape:	 torch.Size([1, 10])
```

结果：

```python
lr, num_epochs, batch_size = 0.1, 20, 64
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231126194752606.png" alt="image-20231126194752606" style="zoom:80%;" />

### batch normalization

**批量规范化（batch normalization）可以加速深层网络的收敛**。如果所有层的学习率都相同，并且没有进行批量规范化，那么每一层输出的数据的方差可能不同，数量级可能有很大差别，这时必须使学习率尽量低。而如果进行了batch-norm，那么可以将学习率调大一些，这样就加速了模型的收敛。

需要注意的是，**批量规范化不能有效加速浅层网络的收敛**。

特点和优势：

1. 在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。
2. 批量规范化在全连接层和卷积层的使用略有不同。
3. 批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。
4. 批量规范化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。

用$\mathbf{x} \in \mathcal{B}$表示一个来自小批量$\mathcal{B}$的输入，批量规范化$\mathrm{BN}$根据以下表达式转换$\mathbf{x}$：

$$
    \mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}
$$

其中，$\hat{\boldsymbol{\mu}}_\mathcal{B}$和${\hat{\boldsymbol{\sigma}}_\mathcal{B}}$，如下所示：

$$
    \begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\
    \hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon.\end{aligned}
$$

$\hat{\boldsymbol{\mu}}_\mathcal{B}$是小批量$\mathcal{B}$的样本均值，$\hat{\boldsymbol{\sigma}}_\mathcal{B}$是小批量$\mathcal{B}$的样本标准差。**拉伸参数（scale）**$\boldsymbol{\gamma}$和**偏移参数（shift）**$\boldsymbol{\beta}$的形状与$\mathbf{x}$相同。**$\boldsymbol{\gamma}$和$\boldsymbol{\beta}$是需要与其他模型参数一起学习的参数**。

batch-norm层的添加和使用有如下三种情况：

1. 全连接层

BN应用在线性变换之后，激活函数之前：

$$
    \mathbf{h} = \phi(\mathrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}) ).
$$

2. 卷积层

假设我们的小批量包含$m$个样本，并且对于每个通道，卷积的输出具有高度$p$和宽度$q$。那么对于卷积层，我们在每个输出通道的$m \cdot p \cdot q$个元素上同时执行每个批量规范化。因此，在计算平均值和方差时，我们会收集所有空间位置的值，然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。**就是分别对各个通道（n个）上的所有的样本和所有的像素求均值，得到的就是n个值，不过维数不变。**

3. 预测时如何使用

在预测时使用全局的均值和标准差进行batch-norm。

代码实现：

```python
'''从0开始实现'''
import torch
from torch import nn
from d2l import torch as d2l

'''batch-norm函数'''
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    '''
    X：输入数据
    gamma：用于学习的参数（当前batch）
    beta：用于学习的参数（当前batch）
    moving_mean：全局参数
    moving_var：全局参数
    eps：常数，防止方差为0
    momentum：用于更新全局参数
    '''
    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        # 
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。
            # 就是分别对各个通道（n个）上的所有的样本和所有的像素求均值，得到的就是n个值，不过维数不变。
            # 这里我们需要保持X的形状以便后面可以做广播运算
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # 训练模式下，用当前的均值和方差做标准化
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 更新移动平均的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # 缩放和移位
    return Y, moving_mean.data, moving_var.data

'''batch-norm层'''
class BatchNorm(nn.Module):
    # num_features：完全连接层的输出数量或卷积层的输出通道数。
    # num_dims：2表示完全连接层，4表示卷积层
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # 非模型参数的变量初始化为0和1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的moving_mean和moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
```

实际应用：

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),
    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),
    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),
    nn.Linear(84, 10))

lr, num_epochs, batch_size = 1.0, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

实际应用时，我们一般可以调用框架提供的函数：

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),
    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),
    nn.Linear(84, 10))
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

### ResNet

特点和优点：

1. 李沐认为可以从以下角度理解ResNet：如果我们新加一个模块，但是这个模块起不到任何作用，那么其梯度可能就接近于0，这样在实际中我们就很容易因为加了不起作用的模块而梯度消失。ResNet通过将输入和输出相加，使得及时输出的梯度接近于0，仍然有大于1的梯度进行反向传播。
2. 为什么要叫残差连接？

输入$x$其实是某一个模块的输出，如果这个输出没有把模型拟合得很好，那么就需要使用剩下的模块来拟合剩下的部分，剩下的那部分就可以称为残差。

#### ResNet块

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231127103340083.png" alt="image-20231127103340083" style="zoom:80%;" />

如果输入数据和输出数据通道数相同，那么可以直接相加，如果不相同，那就将输入x应用一个$1 \times 1$卷积调整通道和分辨率。

代码实现：

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

class Residual(nn.Module):  #@save
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        #是否使用1×1卷积块
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        #因为不同的BatchNorm学习到的参数可能不同，因此要定义2个BatchNorm
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```

使用实例：

```python
blk = Residual(3,3)
X = torch.rand(4, 3, 6, 6)
Y = blk(X)
Y.shape
#输入和输出的形状相同
#torch.Size([4, 3, 6, 6])

blk = Residual(3,6, use_1x1conv=True, strides=2)
blk(X).shape
#输入和输出的形状不同，增加输出通道数的同时，减半输出的高和宽
#torch.Size([4, 6, 3, 3])
```

上面描述的时原始论文中的残差块，后来又有很多新的残差块提出，不同残差块的变体如下：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231126215755998.png" alt="image-20231126215755998" style="zoom:80%;" />

#### ResNet整体模型

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231127105240971.png" alt="image-20231127105240971" style="zoom:80%;" />

代码实现：

```python
'''第一个模块'''
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.BatchNorm2d(64), nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
'''4个残差模块'''
#定义一个函数，后面进行批量添加残差模块
def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    #如果不是第一个模块，那么就会在每个模块中的第一个残差块中使用1×1卷积块
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels, num_channels,
                                use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk #最终返回一个列表
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))

#最后的结果与GooGLeNet类似，都是经过全局最大池化层之后再加一个MLP
net = nn.Sequential(b1, b2, b3, b4, b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(), nn.Linear(512, 10))
```

下面输出ResNet中不同模块的输入形状是如何变化的：

大体思路就是通道数加倍，像素减半

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
#Sequential output shape:	 torch.Size([1, 64, 56, 56])
#Sequential output shape:	 torch.Size([1, 64, 56, 56])
#Sequential output shape:	 torch.Size([1, 128, 28, 28])
#Sequential output shape:	 torch.Size([1, 256, 14, 14])
#Sequential output shape:	 torch.Size([1, 512, 7, 7])
#AdaptiveAvgPool2d output shape:	 torch.Size([1, 512, 1, 1])
#Flatten output shape:	 torch.Size([1, 512])
#Linear output shape:	 torch.Size([1, 10])
```

训练：

```python
lr, num_epochs, batch_size = 0.05, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231127111635914.png" alt="image-20231127111635914" style="zoom:80%;" />

### DenseNet

ResNet将$x$与$f(x)$相加，作为下一个模块的输入。与ResNet类似，DenseNet不仅仅只是将$x+f(x)$作为下一个模块的输入，而是将之前所有模块的输出作为输入。

结构如下所示：

$$
    \mathbf{x} \to \left[
    \mathbf{x},
    f_1(\mathbf{x}),
    f_2([\mathbf{x}, f_1(\mathbf{x})]), f_3([\mathbf{x}, f_1(\mathbf{x}), f_2([\mathbf{x}, f_1(\mathbf{x})])]), \ldots\right]
$$

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231128202745931.png" alt="image-20231128202745931" style="zoom:80%;" />

稠密网络主要由2部分构成：**稠密块（dense block）和过渡层（transition layer）**。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。

1. 稠密块

```python
import torch
from torch import nn
from d2l import torch as d2l

#卷积模块构造：batch-norm + ReLU + 卷积层
def conv_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels), nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))

#定义稠密块

```

2. 过渡层

由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。而过渡层可以用来控制模型复杂度。它通过$1\times 1$卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。

```python
def transition_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels), nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2))
#对上一个例子中稠密块的输出使用通道数为10的过渡层。 此时输出的通道数减为10，高和宽均减半。
blk = transition_block(23, 10)
blk(Y).shape
#torch.Size([4, 10, 4, 4])
```
3. DenseNet模型

DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。

```python
b1 = nn.Sequential(
    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
    nn.BatchNorm2d(64), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

接下来，类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。 与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。 这里我们设成4，从而与ResNet-18保持一致。 稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。

在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。

```python
# num_channels为当前的通道数
num_channels, growth_rate = 64, 32
num_convs_in_dense_blocks = [4, 4, 4, 4]
blks = []
for i, num_convs in enumerate(num_convs_in_dense_blocks):
    blks.append(DenseBlock(num_convs, num_channels, growth_rate))
    # 上一个稠密块的输出通道数
    num_channels += num_convs * growth_rate
    # 在稠密块之间添加一个转换层，使通道数量减半
    if i != len(num_convs_in_dense_blocks) - 1:
        blks.append(transition_block(num_channels, num_channels // 2))
        num_channels = num_channels // 2
```

与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。

```python
net = nn.Sequential(
    b1, *blks,
    nn.BatchNorm2d(num_channels), nn.ReLU(),
    nn.AdaptiveAvgPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(num_channels, 10))
```

```python
lr, num_epochs, batch_size = 0.1, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231128205023531.png" alt="image-20231128205023531" style="zoom:80%;" />

## 循环神经网络

### 序列模型

- 对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。

时间序列预测实际上就是希望利用t时刻之前的信息预测t时刻之后的信息，表示为：**时间步（time step）**$t \in \mathbb{Z}^+$时，观测值为$x_t$。通过以下途径预测$x_t$：

$$
    x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)
$$

在后面的章节中，主要是围绕**如何有效估计$P(x_t \mid x_{t-1}, \ldots, x_1)$**来展开。

#### 自回归模型

1. 第一种策略：当前时刻的观测值与前$\tau$期的观测值有关，利用前$\tau$期的观测值来预测当前期。这种模型被称为**自回归模型（autoregressive models）**。
2. 第二种策略：是保留一些对过去观测的总结$h_t$，并且同时更新预测$\hat{x}_t$和总结$h_t$。这就产生了基于$\hat{x}_t = P(x_t \mid h_{t})$估计$x_t$，以及公式$h_t = g(h_{t-1}, x_{t-1})$更新的模型。由于$h_t$从未被观测到，这类模型也被称为**隐变量自回归模型（latent autoregressive models）**。

整个序列的估计值都将通过以下的方式获得：

$$
    P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1)
$$

如果我们处理的是离散的对象（如单词），而不是连续的数字，那么对于离散的对象，我们需要使用分类器而不是回归模型来估计$P(x_t \mid  x_{t-1}, \ldots, x_1)$。

如何构造自回归模型所需的参数：

```python
#x为时间序列数据
tau = 4 #步长，即当前时刻与之前多少个数据相关
features = torch.zeros((T - tau, tau)) #T为数据的总长度
for i in range(tau):
    features[:, i] = x[i: T - tau + i] #构造特征列
labels = x[tau:].reshape((-1, 1)) #标签列为下一时刻的值
```

### 文本数据预处理

文本的常见预处理步骤：

1. **读取文本数据**：将文本作为字符串加载到内存中。
2. **分词**：将字符串拆分为词元（如单词和字符）。
3. **文本编码**：建立一个词表，将拆分的词元映射到数字索引。
4. **整合**：将文本转换为数字索引序列，方便模型操作。

1. **读取文本数据**

```python
import collections
import re
from d2l import torch as d2l

#@save
d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():  #@save
    """
    将时间机器数据集加载到文本行的列表中
    输出：一个嵌套列表，外层列表为各个不同的句子，里层列表为一句话
    """
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

lines = read_time_machine()
print(f'# 文本总行数: {len(lines)}')
print(lines[0])
print(lines[10])
```

2. **分词**，将文本词元化

```python
def tokenize(lines, token='word'):  #@save
    """
    将文本行拆分为单词或字符词元
    输入：嵌套列表
    输出：嵌套列表，外层列表为各个不同的句子，内层列表为切分好的句子
    可以选择将句子切分为单词或字符词元
    """
    if token == 'word':
        return [line.split() for line in lines]
    #如果是按照字符来切分，就会把每个字母切开，此方法对于中文汉字有一定作用
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
```

3. **文本编码**，为每个字符添加一个编号

```python
class Vocab:  #@save
    """
    文本词表
    输入：一个token或者token组成的列表
    输出：返回token的编号
    """
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens) #频数统计字典
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                   reverse=True) #将字典的key按照values值从大到小排序
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens #先定义一些特殊的字符，包括unk和开始结束字符
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)} #使用enumerate为每一个字符配一个编号（从0开始）
        for token, freq in self._token_freqs:
            #从字典中取出key和values，注意这是排序之后的字典
            if freq < min_freq:
                break #删除频率较低的词元，这可以降低复杂性
            if token not in self.token_to_idx:
                self.idx_to_token.append(token) #将token添加到列表中
                self.token_to_idx[token] = len(self.idx_to_token) - 1 #将token添加到字典中，token作为key，idx_to_token列表的长度作为其编号

    def __len__(self):
        return len(self.idx_to_token)
    
    #类似__xx__的方法会自动运行
    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk) #如果传入的是单个token，就返回其编号
        return [self.__getitem__(token) for token in tokens] #如果传入的是一个列表，就先从列表中挨个取出token，再返回其编号

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property #加了property之后，调用unk就不需要加括号了，否则就一定需要加括号
    def unk(self):  # 未知词元的索引为0
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):  #@save
    """统计词元的频率"""
    # 这里的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成一个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)
```

使用实例：

```python
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[:10])
#[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]
for i in [0, 10]:
    print('文本:', tokens[i])
    print('索引:', vocab[tokens[i]])
#文本: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']
#索引: [1, 19, 50, 40, 2183, 2184, 400]
#文本: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']
#索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]
```

4. **整合**

```python
def load_corpus_time_machine(max_tokens=-1):  #@save
    """返回时光机器数据集的词元索引列表和词表"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens)
    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，
    # 所以将所有文本行展平到一个列表中
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```

### 语言模型和数据集

#### 语言模型

1. 一般语言模型

语言模型就是要学习如下的概率分布：

$$
P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1})
$$

例如，包含了四个单词的一个文本序列的概率是：

$$
P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is})
$$

但是这些概率分布所需要的参数量太大，因此提出了N元语法模型。

2. N元语法模型

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231130211154390.png" alt="image-20231130211154390" style="zoom:80%;" />

下面实现N元语法模型：

```python
import random
import torch
from d2l import torch as d2l

tokens = d2l.tokenize(d2l.read_time_machine())

'''一元语法'''
# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
vocab.token_freqs[:10]
#[('the', 2261),
#('i', 1267),...

'''二元语法'''
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]
#[(('of', 'the'), 309),
#(('in', 'the'), 169),...

'''三元语法'''
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]
#[(('the', 'time', 'traveller'), 59),
#(('the', 'time', 'machine'), 30),...

'''画图'''
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```

从下图可以看出，不管是一元语法、二元语法还是三元语法，绝大部分词（或词组）出现的频数都很小，只有少数词（或词组）出现的频数很大。这说明语言中存在相当多的结构， 这些结构给了我们应用模型的希望。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231130213350612.png" alt="image-20231130213350612" style="zoom:80%;" />

#### 文本采样（构造数据集）

以下两点是与词表占用空间有关的问题

1. 二元词频表所占用的存储空间：

在构造二元词频表的时候，一共会有1000\*1000种组合方式，在制作那个词频表得先把这1000\*1000种所有情况都列举出来。固然我们这里只取文本中先后相邻的二元对，所以显然我们这样得出的词频表是非常稀疏的，大多都是0，但毕竟内存空间还是得消耗这么多。

2. 即使有很多0存在，也要进行结构化存储：

首先，稀疏存储指的是只存储非零元素，对于大部分为零的情况并不存储，这样会导致查找复杂度为O(n)。这是因为在稀疏存储中，要查找一个特定的2元组，需要遍历存储的所有元素，直到找到匹配的2元组。相比之下，哈希存储指的是使用哈希表来存储数据，这样可以将数据存储在不同的位置，从而实现O(1)的查找时间复杂度。在这种情况下，无论存储了多少个2元组，查找所需的时间都是常数级别的，因为可以直接通过哈希函数找到对应的位置，而不需要遍历整个数据集。

下列函数通过控制第一个子序列的随机起始位置来产生随机的子序列，输入文本序列（编号）、批量大小、时间步长，将原始序列分为多个batch，每个batch中包含多个样本（批量大小），每个样本都是X和Y的样本对。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231201210042679.png" alt="image-20231201210042679" style="zoom:80%;" />

```python
def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """
    使用随机抽样生成一个小批量子序列
    输入：文本序列、批量大小、时间步长
    输出：
    """
    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
    corpus = corpus[random.randint(0, num_steps - 1):] #序列前面随机丢掉一部分
    # 减去1，是因为我们需要考虑标签。
    num_subseqs = (len(corpus) - 1) // num_steps #这里计算的是切分出来的子序列个数。
    # 长度为num_steps的子序列的起始索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # 在随机抽样的迭代过程中，
    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
    random.shuffle(initial_indices) #通过将起始索引打乱，从而打乱子序列

    def data(pos):
        # 返回从pos位置开始的长度为num_steps的序列
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size #计算小批量的个数
    for i in range(0, batch_size * num_batches, batch_size):
        # 在这里，initial_indices包含子序列的随机起始索引
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
```

```python
my_seq = list(range(34))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

输出结果如下所示：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231201211048169.png" alt="image-20231201211048169" style="zoom:80%;" />

上面的函数将相邻两个批量的样本顺序打乱了，下面的函数将顺序不打乱。

```python
def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save
    """使用顺序分区生成一个小批量子序列"""
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
```

输出结果如下所示：

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231201211101355.png" alt="image-20231201211101355" style="zoom:80%;" />

### RNN

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231212100454435.png" alt="image-20231212100454435" style="zoom:80%;" />

#### 潜变量自回归模型

潜变量也称为**隐变量（hidden variable）**，由于n元语法模型需要存储大量的参数（各个单词的组合），因此提出使用隐变量来代替条件期望中的条件，使得之前所有的单词都对现在的预测产生影响。即：

$$
P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1})
$$

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231201213320133.png" alt="image-20231201213320133" style="zoom:80%;" />

其中$h_t$可以仅仅存储到目前为止观察到的所有数据，但是这会消耗大量存储空间。下面我们将从其他的角度进行分析，来使得$h_t$存储尽量多的信息，而又不占用太大存储空间。

#### RNN基本思想

首先根据观测值$x_{t-1}$以及因变量$h_{t-1}$来得到隐变量$h_t$，进而达到输出$o_t$，将$o_t$与$x_t$比较即可计算损失。如果去除隐变量，那么剩下的模型就相当于一个MLP。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231201214747774.png" alt="image-20231201214747774" style="zoom:80%;" />

而如果加上隐变量，那么隐变量$H_t$的更新公式就如下：

$$
    \mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)
$$

在这里，隐状态中$\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}$的计算，相当于$\mathbf{X}_t$和$\mathbf{H}_{t-1}$的拼接与$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接的矩阵乘法。

这就使得模型可以捕获历史信息，其中隐变量$H_t$被循环计算，因此将其称为**循环神经网络（recurrent neural network）**。对于时间步$t$，输出层的输出类似于多层感知机中的计算：
$$
    \mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q
$$

在这里，模型参数是$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接，以及$\mathbf{b}_h$的偏置，所有这些参数都来自隐状态。当前时间步$t$的隐状态$\mathbf{H}_t$将参与计算下一时间步$t+1$的隐状态$\mathbf{H}_{t+1}$。而且$\mathbf{H}_t$还将送入全连接输出层，用于计算当前时间步$t$的输出$\mathbf{O}_t$。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231202182459474.png" alt="image-20231202182459474" style="zoom:80%;" />

#### 困惑度（Perplexity）

语言模型通常使用困惑度来衡量模型的好坏，一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。

我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：

$$
    \frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)
$$

其中$P$由语言模型给出，$x_t$是在时间步$t$从该序列中观察到的实际词元。该公式表示每个词语被预测准确的负对数概率平均值，最理想的情况是概率为1，此时困惑度为0，概率越低，交叉熵损失就越大。取平均就是为了防止文本长度不同导致的交叉熵损失不同的问题。

不过由于历史原因，我们使用困惑度来衡量：

$$
    \exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)
$$

#### 梯度剪裁（防止梯度爆炸）

如果g很大，得到的值就是$\theta$，如果g很小，得到的值就是g本身。该函数就保证了g的值不会超过$\theta$。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231201220043749.png" alt="image-20231201220043749" style="zoom:80%;" />

#### 通过时间反向传播

为什么RNN会出现梯度消失和梯度爆炸，见课本第329页。

## 现代循环神经网络

### 门控循环单元GRU

#### 为什么需要GRU？

因为不是每个观察值都是同等重要的。在文本中，只有关键字和关键句重要，标点符号、换行符等字符就不是特别重要了，这些都会对我们的决策产生影响。同时，有可能相关性比较大的两个词元相隔较远，这样隐变量就难以发挥有效的作用。这意味着模型需要有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态。在此我们就需要**遗忘门**和**重置门**。

#### 重置门(reset gate)和更新门(update gate)

- **重置门**有助于捕获序列中的**短期依赖**关系
- **更新门**有助于捕获序列中的**长期依赖**关系

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213100741455.png" alt="image-20231213100741455" style="zoom:80%;" />

$$
\begin{aligned}
    \mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\
    \mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),
    \end{aligned}
$$

其中$\mathbf{W}_{xr}, \mathbf{W}_{xz} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hr}, \mathbf{W}_{hz} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1 \times h}$是偏置参数。这里使用sigmoid函数来将输入值转换到区间（0,1）

#### 候选隐藏状态

接下来，让我们将重置门$\mathbf{R}_t$与常规隐状态更新机制集成，得到在时间步$t$的**候选隐状态（candidate hidden state）**$\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$。

$$
    \tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h)
$$

其中$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$是偏置项，符号$\odot$是Hadamard积（按元素乘积）运算符。在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。使用tanh的主要原因是此时没有出现ReLU。

每当重置门$\mathbf{R}_t$中的项接近$1$时，相当于普通的循环神经网络。对于重置门$\mathbf{R}_t$中所有接近$0$的项，候选隐状态是以$\mathbf{X}_t$作为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被*重置*为默认值。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213102030633.png" alt="image-20231213102030633" style="zoom:80%;" />

#### 隐状态

上述准备完成之后，再确定最终的隐状态。这一步确定新的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$在多大程度上来自旧的状态$\mathbf{H}_{t-1}$和新的候选状态$\tilde{\mathbf{H}}_t$。这就得出了门控循环单元的最终更新公式：

$$
    \mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t
$$

每当更新门$\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。此时，来自$\mathbf{X}_t$的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步$t$。相反，当$\mathbf{Z}_t$接近$0$时，新的隐状态$\mathbf{H}_t$就会接近候选隐状态$\tilde{\mathbf{H}}_t$。这些设计可以帮助我们处理循环神经网络中的梯度消失问题，
并更好地捕获时间步距离很长的序列的依赖关系。例如，如果整个子序列的所有时间步的更新门都接近于$1$，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213103932545.png" alt="image-20231213103932545" style="zoom:80%;" />

QA：

GRU网络中，$R_z$和$Z_t$的结构是一样的，为什么就可以自动把$R_z$学成Reset gate，$Z_t$学成Update gate呢？

期望学成这样，这和attention里面的qkv三矩阵很相似。

### LSTM

#### 输入门、遗忘门、输出门、候选记忆元

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213112005026.png" alt="image-20231213112005026" style="zoom:80%;" />
$$
\begin{aligned}
    \mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\
    \mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\
	\tilde{\mathbf{C}}_t &= \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c),\\
	\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o).
\end{aligned}
$$

#### 记忆元

输入门$\mathbf{I}_t$控制采用多少来自$\tilde{\mathbf{C}}_t$的新数据，而遗忘门$\mathbf{F}_t$控制保留多少过去的记忆元$\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}$的内容。使用按元素乘法，得出：

$$
\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t
$$

如果遗忘门始终为$1$且输入门始终为$0$，则过去的记忆元$\mathbf{C}_{t-1}$将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213151505721.png" alt="image-20231213151505721" style="zoom:80%;" />

#### 隐状态

最后，我们需要定义如何计算隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$，这就是输出门发挥作用的地方。在长短期记忆网络中，它仅仅是记忆元的$\tanh$的门控版本。这就确保了$\mathbf{H}_t$的值始终在区间$(-1, 1)$内：

$$
\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)
$$

只要输出门接近$1$，我们就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近$0$，我们只保留记忆元内的所有信息，而不需要更新隐状态。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213151644671.png" alt="image-20231213151644671" style="zoom:80%;" />

### 深度循环神经网络

* 在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。
* 有许多不同风格的深度循环神经网络，如长短期记忆网络、门控循环单元、或经典循环神经网络。
  这些模型在深度学习框架的高级API中都有涵盖。
* 总体而言，深度循环神经网络需要大量的调参（如学习率和修剪）来确保合适的收敛，模型的初始化也需要谨慎。

下图描述了一个具有$L$个隐藏层的深度循环神经网络，每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213161403528.png" alt="image-20231213161403528" style="zoom:80%;" />

假设在时间步$t$有一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。同时，将$l^\mathrm{th}$隐藏层（$l=1,\ldots,L$）的隐状态设为$\mathbf{H}_t^{(l)}  \in \mathbb{R}^{n \times h}$（隐藏单元数：$h$），输出层变量设为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。设置$\mathbf{H}_t^{(0)} = \mathbf{X}_t$，第$l$个隐藏层的隐状态使用激活函数$\phi_l$，则：

$$
    \mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)})
$$

其中，权重$\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$，$\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$都是第$l$个隐藏层的模型参数。

最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态：

$$
    \mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q
$$

其中，权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$都是输出层的模型参数。

### 双向循环神经网络

因为双向循环视神经网络需要同时知道一个字符的前文和后文，因此它不适合用作预测，比较适合用作文本摘要提取等工作。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213163953865.png" alt="image-20231213163953865" style="zoom:80%;" />

对于任意时间步$t$，给定一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数$n$，每个示例中的输入数$d$），并且令隐藏层激活函数为$\phi$。在双向架构中，我们设该时间步的前向和反向隐状态分别为$\overrightarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$和$\overleftarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$，其中$h$是隐藏单元的数目。

前向和反向隐状态的更新如下：

$$
    \begin{aligned}
    \overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}),\\
    \overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}),
    \end{aligned}
$$

其中，权重$\mathbf{W}_{xh}^{(f)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}_{xh}^{(b)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(f)} \in \mathbb{R}^{1 \times h}, \mathbf{b}_h^{(b)} \in \mathbb{R}^{1 \times h}$都是模型参数。

接下来，将前向隐状态$\overrightarrow{\mathbf{H}}_t$和反向隐状态$\overleftarrow{\mathbf{H}}_t$连接起来，获得需要送入输出层的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。最后，输出层计算得到的输出为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（$q$是输出单元的数目）：

$$
    \mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q
$$

这里，权重矩阵$\mathbf{W}_{hq} \in \mathbb{R}^{2h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的模型参数。事实上，这两个方向可以拥有不同数量的隐藏单元。

### 如何构造机器翻译数据集

- 机器翻译模型是一种序列转换模型。
- 使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。
- 通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。

### encoder-decoder

- “编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。
- 编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
- 解码器将具有固定形状的编码状态映射为长度可变的序列。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231213171406935.png" alt="image-20231213171406935" style="zoom:80%;" />

### seq2seq

#### encoder

编码器将长度可变的输入序列转换成形状固定的上下文变量$\mathbf{c}$，实现信息的编码，可以使用循环神经网络来设计编码器（也可以由 attetion 来设计，还可以选择双向循环神将网络）。编码器通过选定的函数$q$，将所有时间步的隐状态转换为上下文变量：

$$
    \mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T)
$$

当选择$q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T$时，上下文变量仅仅是输入序列在最后时间步的隐状态$\mathbf{h}_T$。

#### decoder

编码器输出的上下文变量$\mathbf{c}$对整个输入序列$x_1, \ldots, x_T$进行编码。来自训练数据集的输出序列$y_1, y_2, \ldots, y_{T'}$，对于每个时间步$t'$，解码器输出$y_{t'}$的概率取决于先前的输出子序列$y_1, \ldots, y_{t'-1}$和上下文变量$\mathbf{c}$，即$P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$。

循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$和上下文变量$\mathbf{c}$作为其输入，然后在当前时间步将它们和上一隐状态$\mathbf{s}_{t^\prime-1}$转换为隐状态$\mathbf{s}_{t^\prime}$。因此，可以使用函数$g$来表示解码器的隐藏层的变换：

$$
    \mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1})
$$

在获得解码器的隐状态之后，我们可以使用输出层和 softmax 操作来计算在时间步$t^\prime$时输出$y_{t^\prime}$的条件概率分布$P(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \mathbf{c})$。

#### encoder与decoder在训练和预测阶段的区别

在训练阶段，encoder和decoder都是RNN，但是encoder中所有的输入都是相等长度的字符，将encoder的最后一个隐藏状态输入给decoder，同时将开始符\<bos>输入给decoder，decoder没有固定的长度，直到见到\<eos>停止符才停止。

不过训练阶段和预测阶段略有不同，训练阶段的decoder中，**每个cell的输出并不作为下一个cell的输入，而是使用真实值最为其输入**，因为训练阶段的预测有很大误差。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231214200800750.png" alt="image-20231214200800750" style="zoom:80%;" />

预测阶段，**decoder中每个cell的输出都会最为下一个cell的输入**。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231214202109945.png" alt="image-20231214202109945" style="zoom:80%;" />

#### seq2seq的评价标准BLEU

我们将BLEU（bilingual evaluation understudy）定义为：

$$
    \exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n}
$$

其中$\mathrm{len}_{\text{label}}$表示标签序列中的词元数和$\mathrm{len}_{\text{pred}}$表示预测序列中的词元数，$k$是用于匹配的最长的$n$元语法。另外，用$p_n$表示$n$元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的$n$元语法的数量，第二个是预测序列中$n$元语法的数量的比率。具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$、$B$、$C$、$D$，我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$。

当预测序列与标签序列完全相同时，BLEU为$1$。此外，由于$n$元语法越长则匹配难度越大，所以BLEU为更长的$n$元语法的精确度分配更大的权重。具体来说，当$p_n$固定时，$p_n^{1/2^n}$会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）。而且，由于预测的序列越短获得的$p_n$值越高，所以公式中乘法项之前的系数用于惩罚较短的预测序列。例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，惩罚因子$\exp(1-6/2) \approx 0.14$会降低BLEU。

### 束搜索beam-search

上文提到使用上一步的预测词元作为输入，来预测下一个词元。预测出的词元是以概率最大为标准，但是每一步的概率最大可能并不是全局的概率最大化。因此就有下面这些搜索算法：

- 序列搜索策略包括贪心搜索、穷举搜索和束搜索。
- 贪心搜索所选取序列的计算量最小，但精度相对较低。
- 穷举搜索所选取序列的精度最高，但计算量最大。
- 束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。

**束搜索**刚开始选择k个概率最大的，之后每一步都选择1个概率最大的（贪心算法）

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231214210217155.png" alt="image-20231214210217155" style="zoom:80%;" />

将每一步的条件概率相乘，选择其中条件概率乘积最高的序列作为输出序列：

$$
\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})
$$

其中$L$是最终候选序列的长度，$\alpha$通常设置为$0.75$，$L^\alpha$用于惩罚长序列。

## attention-mechanism

### 注意力评分函数

#### 基本思想

*注意力评分函数*（attention scoring function），简称*评分函数*（scoring function），可以是高斯核。然后把这个函数的输出结果输入到softmax函数中进行运算。通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。

<img src="https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20231216133625860.png" alt="image-20231216133625860" style="zoom:80%;" />

用数学语言描述，假设有一个查询$\mathbf{q} \in \mathbb{R}^q$和$m$个“键－值”对$(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$，其中$\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$。注意力汇聚函数$f$就被表示成值的加权和：

$$
    f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v
$$

其中查询$\mathbf{q}$和键$\mathbf{k}_i$的注意力权重（标量）是通过注意力评分函数$a$将两个向量映射成标量，再经过softmax运算得到的：

$$
    \alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}
$$

`注意：由于输入数据并不完全有用，有些数据可能是填充部分，这些部分不需要计算注意力，因此赋予这些部分一个很小的值，最终经过softmax会得到0`

#### 加性注意力

当查询和键是不同长度的矢量时，可以使用**加性注意力（additive attention）**作为评分函数。给定查询$\mathbf{q} \in \mathbb{R}^q$和键$\mathbf{k} \in \mathbb{R}^k$，*加性注意力*（additive attention）的评分函数为
$$
    a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}
$$

其中可学习的参数是$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$和$\mathbf w_v\in\mathbb R^{h}$。此处$a(\mathbf q, \mathbf k)$为单个值，而不是矩阵或向量。

将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$。通过使用$\tanh$作为激活函数，并且禁用偏置项。

#### 缩放点积注意力

使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有**相同的长度**$d$。假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为$0$，方差为$d$。为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是$1$，我们再将点积除以$\sqrt{d}$，则**缩放点积注意力（scaled dot-product attention）**评分函数为：

$$
    a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}
$$

在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键－值对计算注意力，其中查询和键的长度为$d$，值的长度为$v$。查询$\mathbf Q\in\mathbb R^{n\times d}$、键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$的缩放点积注意力是：

$$
\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}
$$

#### 二者之间的优劣

*加性注意力*可以不需要q和k的长度相等，但是计算过程中需要学习超参数。
*缩放点积注意力*不需要学习超参数，但是需要q和k的长度相等。

### bahdanau-attention

本小节阐述注意力机制如何在seq2seq模型中使用。bahdanau-attention 实际上就是attention 与 RNN 的结合。

#### 使用隐变量的弊端

![image-20240104101500195](https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20240104101500195.png)

- 在机器翻译的时候，每个生成的词可能相关于源句子中不同的词。
- 源句子中的所有信息虽然都包含在这个隐藏状态中，但是要想在翻译某个词的时候，每个解码步骤使用编码相同的上下文变量，但是**并非所有输入（源）词元都对解码某个词元有用**。
- 然而，Seq2Seq 模型中不能对此直接建模。Seq2Seq 模型中编码器向解码器中传递的信息是编码器**最后时刻的隐藏状态**，解码器只用到了编码器最后时刻的隐藏状态作为初始化，从而进行预测，所以解码器看不到编码器中除最后时刻之外的其他隐状态。
- 因此，将注意力关注在源句子中的对应位置，这也是将注意力机制应用在Seq2Seq 模型中的动机。

#### 加入注意力机制

![image-20240104100527917](https://raw.githubusercontent.com/zhangyuanwang777/Picture-cloud/main/img/image-20240104100527917.png)

- 编码器对每次词的输出（隐藏状态）作为 key 和 value，因此 **key 和 value 是等价的**。序列中有多少个词元，就有多少个 key-value 对。
- 解码器 RNN 对上一个词的预测输出（隐藏状态）是 query。
在此，假设 RNN 的输出都是在同一个语义空间中，所以在解码器中对某个词元进行解码的时候，需要用到的是 RNN 的输出，而不能使用词嵌入之后的输入，因为 key 和 value 也是 RNN 的输出，所以 key 和 query 做匹配的时候，最好都使用 RNN 的输出，这样能够保证它们差不多在同一个语义空间。
- 注意力的输出和下一个词的词嵌入合并进入 RNN 解码器 。
- 对 Seq2Seq 的改进之处在于：之前 Seq2Seq 的 RNN 解码器的输入是 RNN 编码器最后时刻的隐藏状态，加入注意力机制之后的模型相当于是对所有的词进行了加权平均，根据翻译的词的不同使用不同时刻的 RNN 编码器输出的隐藏状态。

### multihead-attention



### self-attention and positional-encoding



### transformer



## 优化算法



## 高性能计算



## CV

### 数据增强

通过对原始数据集进行旋转、剪切、变色等操作来给数据添加噪声，以保证模型在实际应用过程中有很好的效果。这种方式可以看做**正则化**，增强模型的**泛化性**。不过如果预计在实际应用的过程当中，实际数据和训练数据相差不大的话，可以不考虑对原始数据进行变换。



## NLP



